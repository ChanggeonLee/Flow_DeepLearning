{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ichang-geon/.pyenv/versions/2.7.14/envs/flow/lib/python2.7/site-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/ichang-geon/.pyenv/versions/2.7.14/envs/flow/lib/python2.7/site-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n",
      "/Users/ichang-geon/.pyenv/versions/2.7.14/envs/flow/lib/python2.7/site-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob as glb\n",
    "from tqdm import *\n",
    "import h5py\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "min_queue_examples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flow(filename, shape):\n",
    "  stream_flow = h5py.File(filename, 'r')\n",
    "  flow_state_vel = np.array(stream_flow['Velocity_0'][:])\n",
    "  flow_state_vel = flow_state_vel.reshape([shape[0], shape[1]+128, 3])[0:shape[0],0:shape[1],0:2]\n",
    "  stream_flow.close()\n",
    "  return flow_state_vel\n",
    "\n",
    "def load_boundary(filename, shape):\n",
    "  stream_boundary = h5py.File(filename, 'r')\n",
    "  boundary_cond = np.array(stream_boundary['Gamma'][:])\n",
    "  boundary_cond = boundary_cond.reshape([shape[0], shape[1]+128, 1])[0:shape[0],0:shape[1],:]\n",
    "  stream_boundary.close()\n",
    "  return boundary_cond\n",
    "\n",
    "def read_data(filename_queue, shape):\n",
    "  reader = tf.TFRecordReader()\n",
    "  key, serialized_example = reader.read(filename_queue)\n",
    "  features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "      'boundary':tf.FixedLenFeature([],tf.string),\n",
    "      'sflow':tf.FixedLenFeature([],tf.string),\n",
    "      'vmax':tf.FixedLenFeature([],tf.string),\n",
    "    }) \n",
    "  boundary = tf.decode_raw(features['boundary'], tf.uint8)\n",
    "  sflow = tf.decode_raw(features['sflow'], tf.float32)\n",
    "  vmax = tf.decode_raw(features['vmax'], tf.float32)\n",
    "\n",
    "  boundary = tf.reshape(boundary, [shape[0], shape[1], 1])\n",
    "  sflow = tf.reshape(sflow, [shape[0], shape[1], 2])\n",
    "  vmax = tf.reshape(vmax, [1024])\n",
    "  boundary = tf.to_float(boundary)\n",
    "  sflow = tf.to_float(sflow)\n",
    "  vmax = tf.to_float(vmax)\n",
    "\n",
    "  return boundary, sflow, vmax\n",
    "\n",
    "def _generate_image_label_batch(boundary, sflow, vmax, batch_size, shuffle=True):\n",
    "  num_preprocess_threads = 1\n",
    "  #Create a queue that shuffles the examples, and then\n",
    "  #read 'batch_size' images + labels from the example queue.\n",
    "  boundarys, sflows, vmax = tf.train.shuffle_batch(\n",
    "    [boundary, sflow, vmax],\n",
    "    batch_size=batch_size,\n",
    "    num_threads=num_preprocess_threads,\n",
    "    capacity=min_queue_examples + 3 * batch_size,\n",
    "    min_after_dequeue=min_queue_examples)\n",
    "  return boundarys, sflows, vmax\n",
    "\n",
    "def flow_inputs(batch_size):\n",
    "  shape = (128,256)\n",
    "\n",
    "  tfrecord_filename = glb('./data/*.tfrecords') \n",
    "  \n",
    "  filename_queue = tf.train.string_input_producer(tfrecord_filename) \n",
    "\n",
    "  boundary, sflow,vmax = read_data(filename_queue, shape)\n",
    "\n",
    "  boundarys, sflows, vmax = _generate_image_label_batch(boundary, sflow,vmax, batch_size)\n",
    "\n",
    "  return boundarys, sflows, vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"shuffle_batch:2\", shape=(8, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "n_batch = 8\n",
    "learning_rate = 0.0001\n",
    "keep_prob = 0.7\n",
    "n_hidden = 128\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "X = tf.placeholder(tf.float32 , [None,128,256,1], name=\"X\")\n",
    "\n",
    "boundary, sflow, vmax = flow_inputs(n_batch)\n",
    "X = boundary\n",
    "\n",
    "v = tf.placeholder(tf.float32 ,[n_batch , 1024], name=\"v\")\n",
    "# v = tf.to_float(vmax)\n",
    "v = vmax\n",
    "\n",
    "print(v)\n",
    "\n",
    "#Conv1\n",
    "W1 = tf.Variable(tf.random_normal([16, 16, 1, 128], stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X, W1, strides=[1, 8, 16, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cov2\n",
    "W2 = tf.Variable(tf.random_normal([4,4,128,512], stddev=0.01))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 4, 4, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_flat = tf.reshape(L2,[-1,4*4*512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = tf.get_variable(\"W\", shape=[512 * 4 * 4, 1024],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout_2/mul:0\", shape=(8, 1024), dtype=float32)\n",
      "Tensor(\"concat:0\", shape=(8, 2048), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "b3 = tf.Variable(tf.random_normal([1024]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2_flat, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "print(L3)\n",
    "# L3 = tf.add(L3 , v)\n",
    "L3=tf.concat([L3, v], 1)\n",
    "print(L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L3 = tf.divide(L3,y)\n",
    "S1, S2 = tf.split(L3, [1024, 1024], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1= tf.reshape(S1,[n_batch,1,1,1024])\n",
    "S2= tf.reshape(S2,[n_batch,1,1,1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deconv1\n",
    "W4_1 = tf.Variable(tf.random_normal([8,8,512,1024], stddev=0.01))\n",
    "L4_1 = tf.nn.conv2d_transpose(S1,W4_1,output_shape=[n_batch,8,8,512],strides=[1,8, 8, 1], padding='SAME')\n",
    "L4_1 = tf.nn.relu(L4_1)\n",
    "L4_1 = tf.nn.dropout(L4_1, keep_prob)\n",
    "\n",
    "W4_2 = tf.Variable(tf.random_normal([8,8,512,1024], stddev=0.01))\n",
    "L4_2 = tf.nn.conv2d_transpose(S2,W4_2,output_shape=[n_batch,8,8,512],strides=[1,8, 8, 1], padding='SAME')\n",
    "L4_2 = tf.nn.relu(L4_2)\n",
    "L4_2 = tf.nn.dropout(L4_2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deconv2\n",
    "W5_1 = tf.Variable(tf.random_normal([4,8,256,512], stddev=0.01))\n",
    "L5_1 = tf.nn.conv2d_transpose(L4_1,W5_1,output_shape=[n_batch,32,64,256],strides=[1, 4, 8, 1], padding='SAME')\n",
    "L5_1 = tf.nn.relu(L5_1)\n",
    "L5_1 = tf.nn.dropout(L5_1, keep_prob)\n",
    "\n",
    "W5_2 = tf.Variable(tf.random_normal([4,8,256,512], stddev=0.01))\n",
    "L5_2 = tf.nn.conv2d_transpose(L4_2,W5_2,output_shape=[n_batch,32,64,256],strides=[1,4, 8, 1], padding='SAME')\n",
    "L5_2 = tf.nn.relu(L5_2)\n",
    "L5_2 = tf.nn.dropout(L5_2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deconv3\n",
    "W6_1 = tf.Variable(tf.random_normal([2,2,32,256], stddev=0.01))\n",
    "L6_1 = tf.nn.conv2d_transpose(L5_1,W6_1,output_shape=[n_batch,64,128,32],strides=[1,2, 2, 1], padding='SAME')\n",
    "L6_1 = tf.nn.relu(L6_1)\n",
    "L6_1 = tf.nn.dropout(L6_1, keep_prob)\n",
    "\n",
    "W6_2 = tf.Variable(tf.random_normal([2,2,32,256], stddev=0.01))\n",
    "L6_2 = tf.nn.conv2d_transpose(L5_2,W6_2,output_shape=[n_batch,64,128,32],strides=[1,2, 2, 1], padding='SAME')\n",
    "L6_2 = tf.nn.relu(L6_2)\n",
    "L6_2 = tf.nn.dropout(L6_2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deconv4\n",
    "W7_1 = tf.Variable(tf.random_normal([2,2,1,32], stddev=0.01))\n",
    "L7_1 = tf.nn.conv2d_transpose(L6_1,W7_1,output_shape=[n_batch,128,256,1],strides=[1,2,2, 1], padding='SAME')\n",
    "L7_1 = tf.nn.dropout(L7_1, keep_prob)\n",
    "\n",
    "W7_2 = tf.Variable(tf.random_normal([2,2,1,32], stddev=0.01))\n",
    "L7_2 = tf.nn.conv2d_transpose(L6_2,W7_2,output_shape=[n_batch,128,256,1],strides=[1,2, 2, 1], padding='SAME')\n",
    "L7_2 = tf.nn.dropout(L7_2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_6:0\", shape=(8, 128, 256, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sflow_p = tf.stack([L7_1 , L7_2] , axis=3)\n",
    "sflow_p = tf.reshape(sflow_p , [n_batch,128,256,2])\n",
    "# loss = tf.reduce_mean(tf.square(sflow_p - sflow))\n",
    "# sflow_p = tf.add(L7_1 , L7_2 )\n",
    "\n",
    "loss = tf.nn.l2_loss(sflow_p - sflow)\n",
    "print(sflow_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#loss \n",
    "total_loss = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.get_checkpoint_state('./model_save')\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-input_producer-input_producer/input_producer_EnqueueMany, started daemon 123145369161728)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145373368320)>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = sess.run([v],feed_dict={})\n",
    "# print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', '0001', 'Avg. cost =', '438.202789')\n",
      "('Epoch:', '0002', 'Avg. cost =', '316.635010')\n",
      "('Epoch:', '0003', 'Avg. cost =', '202.932526')\n",
      "('Epoch:', '0004', 'Avg. cost =', '589.463135')\n",
      "('Epoch:', '0005', 'Avg. cost =', '451.263489')\n",
      "('Epoch:', '0006', 'Avg. cost =', '693.034180')\n",
      "('Epoch:', '0007', 'Avg. cost =', '394.903625')\n",
      "('Epoch:', '0008', 'Avg. cost =', '373.835022')\n",
      "('Epoch:', '0009', 'Avg. cost =', '733.387207')\n",
      "('Epoch:', '0010', 'Avg. cost =', '317.088501')\n",
      "최적화 완료\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./model_save/model.ckpt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = 10\n",
    "\n",
    "# min_cost = 10000\n",
    "for epoch in range(steps):    \n",
    "  total_cost = 0\n",
    "  _, cost_val = sess.run([total_loss, loss],feed_dict={})\n",
    "  total_cost += cost_val\n",
    "\n",
    "  print('Epoch:', '%04d' % (epoch + 1),\n",
    "        'Avg. cost =', '{:f}'.format(total_cost))\n",
    "print(\"최적화 완료\") \n",
    "saver.save(sess, \"./model_save/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
